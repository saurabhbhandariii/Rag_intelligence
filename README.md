# ğŸ“„ Production-Grade Conversational RAG (PDF Chat)

A **production-oriented Retrieval-Augmented Generation (RAG)** system built with  
**Streamlit, ChromaDB, Ollama, and LangChain**.

Upload PDFs, ask conversational questions, and receive **grounded answers strictly from your documents** â€” fully **offline and local**.

---

## ğŸš€ Features

- ğŸ“„ PDF ingestion & chunking
- ğŸ” Vector search using ChromaDB
- ğŸ§  Local embeddings via Ollama (`nomic-embed-text`)
- ğŸ” Query rewriting using conversation history
- ğŸ“Š Cross-encoder re-ranking
- ğŸ’¬ Conversational memory (last 5 turns)
- ğŸ§¾ Strict context-grounded answers (no hallucinations)
- âš¡ Streaming responses
- ğŸ  Fully offline / local (no cloud APIs)

---

## ğŸ§± Architecture Overview

PDF Upload
â†“
PyMuPDF Loader
â†“
Text Chunking
â†“
Ollama Embeddings
â†“
ChromaDB (Persistent Vector Store)
â†“
Query Rewrite (LLM)
â†“
Vector Retrieval
â†“
Cross-Encoder Re-Ranking
â†“
Context Injection
â†“
Answer Generation (Streaming)


---

## ğŸ›  Tech Stack

| Layer | Technology |
|-----|-----------|
| UI | Streamlit |
| LLM | Ollama (`llama3.2:3b`) |
| Embeddings | Ollama (`nomic-embed-text`) |
| Vector DB | ChromaDB |
| PDF Loader | PyMuPDF |
| Chunking | LangChain |
| Re-Ranking | SentenceTransformers |
| Language | Python |

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/saurabhbhandariii/production-rag-streamlit.git
cd production-rag-streamlit
2ï¸âƒ£ Create virtual environment
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
3ï¸âƒ£ Install dependencies
pip install -r requirements.txt
ğŸ§  Ollama Setup (Required)
Install Ollama from:
ğŸ‘‰ https://ollama.com

Start Ollama:

ollama serve
Pull required models:

ollama pull llama3.2:3b
ollama pull nomic-embed-text
â–¶ï¸ Run the Application
streamlit run app.py
Open in browser:

http://localhost:8501
ğŸ§ª How the RAG Pipeline Works
1. Document Ingestion
PDFs loaded using PyMuPDF

Chunked using recursive text splitting

2. Embedding & Storage
Each chunk embedded locally using Ollama

Stored in persistent ChromaDB

3. Query Rewrite
Converts follow-up questions into standalone queries

Uses conversation history

4. Retrieval
Vector similarity search

Distance threshold filtering

5. Re-Ranking
Cross-encoder ranks top relevant chunks

Improves answer precision

6. Answer Generation
Injects:

System prompt

Conversation history

Retrieved context

Model answers only from provided context

ğŸ” Grounding Rules
The system prompt enforces:

âŒ No hallucinations

âŒ No external knowledge

âœ… Answers only from retrieved documents

âœ… Clear fallback when info is missing
